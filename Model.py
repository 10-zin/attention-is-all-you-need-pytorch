import torch
import torch.nn as nn

def MultiHeadAttention(nn.Module):
    def __init__(self, hid_dim):
        super(MultiHeadAttention, self).__init__()

    def forward(self, x):
        return x


def PositionwiseFeedForward(nn.Module):
    def __init__(self, hid_dim):
        super(PositionwiseFeedForward, self).__init__()

    def forward(self, x):
        return x

def EncoderLayer(nn.Module):

    def __init__(self, hid_dim):
        super(EncoderLayer, self).__init__()

    def forward(self, _Y):
        return x

def DecoderLayer(nn.Module):

    def __init__(self, hid_dim):
        super(DecoderLayer, self).__init__()

    def forward(self, x):
        return x

def Transformer(nn.Module):

    def __init__(self, hid_dim):
        super(EncoderLayer, self).__init__()

    def forward(self, _Y):
        hid_dim = self.hid_dim

